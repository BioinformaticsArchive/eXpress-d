"""Configuration options for running Express-D"""

import time

from config_utils import FlagSet
from config_utils import JavaOptionSet
from config_utils import OptionSet

# --------------------------------------------------------------------------------------------------
# Spark + Express-D Runtime Options
#
# Note: these are actually Java options that will be passed to the SCALA_CMD call. Calling them
#       Java options are a bit confusing...
# --------------------------------------------------------------------------------------------------

# Global options specific to Spark.
SPARK_RUNTIME_OPTS = [
    # Fraction of JVM memory used for caching RDDs.
    JavaOptionSet("spark.storage.memoryFraction", [0.66]),
    JavaOptionSet("spark.serializer", ["spark.JavaSerializer"]),
    # How much memory to give the Spark process running on each slave.
    JavaOptionSet("spark.executor.memory", ["4g"]),
]

# Global options specific to Express-D.
EXPRESS_RUNTIME_OPTS = [
    JavaOptionSet("express.persist.serialize", "false"),
    JavaOptionSet("express.outputInterval", 5000),
    JavaOptionSet("express.targetParallelism", 20)
]

# Options local to each Express-D run.
EXPRESS_RUNTIME_LOCAL_OPTS = [
    OptionSet("hits-file-path", ),
    OptionSet("targets-file-path", ),
    OptionSet("should-use-bias", ),
    OptionSet("num-iterations", ),
    OptionSet("should-cache", ),
    OptionSet("num-partitions-for-alignments", ),
    OptionSet("debug-output", "false")
]

# --------------------------------------------------------------------------------------------------
# Spark Build Configuration Options
# --------------------------------------------------------------------------------------------------

# Required. Location of Spark sources.
SPARK_HOME = "/root/spark"
SPARK_CONF_DIR = SPARK_HOME + "/conf"

# If true, don't build Spark. This assumes that SPARK_HOME has been assembled using
# 'sbt/sbt assembly'.
SPARK_SKIP_PREP = false

# Only applicable is SPARK_SKIP_BUILD is false. The git repository used to clone copies of Spark to
# path/to/express-D/spark.
SPARK_GIT_REPO = "git://github.com/mesos/spark.git -b branch-0.7"

# --------------------------------------------------------------------------------------------------
# General Configuration Options
# --------------------------------------------------------------------------------------------------
SCALA_CMD = "scala"

# This default setting assumes we are running on the Spark EC2 AMI. Users will probably want
# to change this to SPARK_CLUSTER_URL = "spark://localhost:7077" for running locally.
# SPARK_CLUSTER_URL = open("/root/spark-ec2/cluster-url", 'r').readline().strip()
SPARK_CLUSTER_URL = "spark://localhost:7077"
